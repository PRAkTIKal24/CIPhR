name: CIPhR Daily Research Ingestion (Hybrid)

# NOTE: This workflow requires the following secrets to be set:
# - PROD_BOT_KEY: SSH key for pushing to repository
# - GEMINI_API_KEY: Google Gemini API key for LLM analysis
# - MM_WEBHOOK_URL: Mattermost incoming webhook URL for ML4DM notifications (optional)
# 
# WordPress Integration (optional - workflow skips gracefully if not configured):
# - WP_USERNAME: WordPress username for iDMEU integration (required for WordPress)
# - WP_APP_PASSWORD: WordPress application password for iDMEU integration (required for WordPress) 
# - WP_SITE_URL: WordPress site URL (optional, defaults to https://www.idmeu.org)

on:
  schedule:
    # Runs every day at 7:00 AM UK time (7:00 UTC in winter, 8:00 AM in summer)
    - cron: '0 7 * * *'
  workflow_dispatch: # Allows manual triggering

jobs:
  collect_data:
    runs-on: ubuntu-latest
    outputs:
      papers-found: ${{ steps.data-collection.outputs.papers-found }}

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        ssh-key: ${{secrets.PROD_BOT_KEY}}

    - name: Install uv
      uses: astral-sh/setup-uv@v3
      with:
        version: "latest"
        
    - name: Set up Python
      run: uv python install 3.11
        
    - name: Install dependencies
      run: |
        uv sync --refresh
        
    - name: Run Data Collection
      id: data-collection
      run: |
        cd ${{ github.workspace }}
        uv run ciphr --mode collect --max_results 10 --output_filename hepex.md --tags hep-ex --verbose
        
        # Set output based on whether papers_data.json was created and has content
        if [ -f "output/papers_data.json" ]; then
          PAPER_COUNT=$(python3 -c "import json; data=json.load(open('output/papers_data.json')); print(len(data))" 2>/dev/null || echo "0")
          echo "papers-found=$PAPER_COUNT" >> $GITHUB_OUTPUT
          echo "Found $PAPER_COUNT papers for analysis"
        else
          echo "papers-found=0" >> $GITHUB_OUTPUT
          echo "No papers found"
        fi

    - name: Upload collected data
      if: steps.data-collection.outputs.papers-found > 0
      uses: actions/upload-artifact@v4
      with:
        name: collected-data
        path: |
          output/papers_data.json
          output/analysis_prompts.json

  analyze_papers:
    needs: collect_data
    runs-on: ubuntu-latest
    if: needs.collect_data.outputs.papers-found > 0

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        ssh-key: ${{secrets.PROD_BOT_KEY}}

    - name: Download collected data
      uses: actions/download-artifact@v4
      with:
        name: collected-data
        path: output/

    - name: Analyze papers dynamically with Gemini
      env:
        GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
      run: |
        # Load papers and analyze each one dynamically
        python << 'EOF'
        import json
        import os
        import subprocess
        import tempfile

        # Get API key from environment
        api_key = os.getenv('GEMINI_API_KEY')
        if not api_key:
            print("ERROR: GEMINI_API_KEY not found in environment")
            exit(1)

        # Load papers data
        with open('output/papers_data.json', 'r') as f:
            papers = json.load(f)

        print(f"Processing {len(papers)} papers individually...")

        results = []
        
        for i, paper in enumerate(papers):
            print(f"Analyzing paper {i+1}/{len(papers)}: {paper['title'][:50]}...")
            
            # Create individual paper file for this analysis
            individual_paper = {
                "title": paper["title"],
                "abstract": paper.get("abstract", ""),
                "combined_content": paper.get("combined_content", "")[:2000],  # Limit content size
                "arxiv_url": paper["arxiv_url"]
            }
            
            # Write paper to temporary file
            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                json.dump(individual_paper, f, indent=2)
                paper_file = f.name

            # Load all questions for CI analysis (table questions + ML4DM detection)
            import sys
            sys.path.append('.')
            from ciphr.config.config import get_ci_analysis_questions
            prompt_questions = get_ci_analysis_questions()
            
            prompt_intro = "Analyze this physics research paper and answer the following questions. Respond with a JSON object with the exact question text as keys.\\n\\nQuestions:\\n"
            for i, question in enumerate(prompt_questions, 1):
                prompt_intro += f'{i}. "{question}"\\n'
            prompt_intro += "\\n"
            
            prompt_format = "Response format:\\n{\\n"
            for q in prompt_questions:
                prompt_format += f'  "{q}": "Your answer here",\\n'
            prompt_format = prompt_format.rstrip(",\\n") + "\\n}\\n\\nPaper to analyze:\\n"
            
            prompt = prompt_intro + prompt_format + json.dumps(individual_paper, indent=2)

            # Write prompt to file
            with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
                f.write(prompt)
                prompt_file = f.name

            try:
                # Call Gemini API using curl
                api_url = f'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key={api_key}'
                result = subprocess.run([
                    'curl', '-X', 'POST', api_url,
                    '-H', 'Content-Type: application/json',
                    '-d', json.dumps({
                        "contents": [{
                            "parts": [{"text": prompt}]
                        }]
                    })
                ], capture_output=True, text=True, timeout=60)

                if result.returncode == 0:
                    api_response = json.loads(result.stdout)
                    if 'candidates' in api_response and len(api_response['candidates']) > 0:
                        content = api_response['candidates'][0]['content']['parts'][0]['text']
                        results.append(content.strip())
                        print(f"Successfully analyzed paper {i+1}")
                    else:
                        fallback_result = {
                            q: f"API Error: No candidates in response" for q in prompt_questions
                        }
                        results.append(json.dumps(fallback_result))
                        print(f"API error for paper {i+1}: No candidates")
                else:
                    fallback_result = {
                        q: f"API Error: {result.stderr[:100]}" for q in prompt_questions
                    }
                    results.append(json.dumps(fallback_result))
                    print(f"API call failed for paper {i+1}: {result.stderr}")

            except Exception as e:
                print(f"Exception analyzing paper {i+1}: {e}")
                fallback_result = {
                    q: f"Error during analysis: {str(e)[:100]}" for q in prompt_questions
                }
                results.append(json.dumps(fallback_result))

            finally:
                # Clean up temporary files
                try:
                    os.unlink(paper_file)
                    os.unlink(prompt_file)
                except:
                    pass

        # Save combined results
        with open('output/llm_results.txt', 'w') as f:
            f.write("---PAPER---".join(results))

        print(f"Saved combined results for {len(results)} papers")
        
        # Debug output
        print("=== Combined Results Preview ===")
        with open('output/llm_results.txt', 'r') as f:
            content = f.read()
            print(content[:500] + "..." if len(content) > 500 else content)
        print("=== End Preview ===")
        EOF

    - name: Upload analysis results
      uses: actions/upload-artifact@v4
      with:
        name: analysis-results
        path: |
          output/papers_data.json
          output/analysis_prompts.json
          output/llm_results.txt

  process_results:
    needs: [collect_data, analyze_papers]
    runs-on: ubuntu-latest
    if: needs.collect_data.outputs.papers-found > 0

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        ssh-key: ${{secrets.PROD_BOT_KEY}}

    - name: Install uv
      uses: astral-sh/setup-uv@v3
      with:
        version: "latest"
        
    - name: Set up Python
      run: uv python install 3.11
        
    - name: Install dependencies
      run: |
        uv sync --refresh
        # Install pandoc for robust markdown to HTML conversion
        sudo apt-get update
        sudo apt-get install -y pandoc

    - name: Download analysis results
      uses: actions/download-artifact@v4
      with:
        name: analysis-results
        path: output/

    - name: Process results and generate markdown
      run: |
        # Run result processing phase
        uv run ciphr --mode process --output_dir output --output_filename hepex.md --verbose

    - name: Check for ML4DM papers and prepare Mattermost notification
      id: ml4dm-check
      run: |
        # Run ML4DM analysis
        uv run python << 'EOF'
        import json
        import os
        import sys
        from ciphr.src.mattermost_notifier import check_ml4dm_papers, MattermostNotifier

        # Load papers data
        with open('output/papers_data.json', 'r') as f:
            papers_data = json.load(f)

        # Load LLM results
        with open('output/llm_results.txt', 'r') as f:
            content = f.read().strip()

        # Parse LLM results (same logic as in ciphr_hybrid.py)
        PAPER_SEPARATOR = "---PAPER---"
        if PAPER_SEPARATOR in content:
            llm_results = content.split(PAPER_SEPARATOR)
            llm_results = [result.strip() for result in llm_results if result.strip()]
        else:
            llm_results = [content]

        # Ensure we have the right number of results
        if len(llm_results) < len(papers_data):
            while len(llm_results) < len(papers_data):
                llm_results.append('{"error": "No LLM result available"}')
        elif len(llm_results) > len(papers_data):
            llm_results = llm_results[:len(papers_data)]

        print(f"Analyzing {len(papers_data)} papers for ML4DM usage...")

        # Check for ML4DM papers
        ml4dm_papers = check_ml4dm_papers(papers_data, llm_results)

        # Save results
        with open('output/ml4dm_papers.json', 'w') as f:
            json.dump(ml4dm_papers, f, indent=2)

        # Set output for next step
        has_ml4dm = len(ml4dm_papers) > 0
        print(f"Found {len(ml4dm_papers)} ML4DM papers")
        
        # Write to GitHub Actions output
        with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
            f.write(f"has-ml4dm-papers={'true' if has_ml4dm else 'false'}\n")
            f.write(f"ml4dm-count={len(ml4dm_papers)}\n")

        if has_ml4dm:
            print("ML4DM papers found - will post to Mattermost")
        else:
            print("No ML4DM papers found - skipping Mattermost notification")
        EOF

    - name: Post to Mattermost if ML4DM papers found
      if: steps.ml4dm-check.outputs.has-ml4dm-papers == 'true'
      env:
        MM_WEBHOOK_URL: ${{ secrets.MM_WEBHOOK_URL }}
      run: |
        uv run python << 'EOF'
        import json
        import os
        import sys
        from ciphr.src.mattermost_notifier import MattermostNotifier

        # Get webhook URL from environment
        webhook_url = os.getenv('MM_WEBHOOK_URL')
        if not webhook_url:
            print("ERROR: MM_WEBHOOK_URL not found in environment")
            sys.exit(1)

        # Load ML4DM papers
        with open('output/ml4dm_papers.json', 'r') as f:
            ml4dm_papers = json.load(f)

        if not ml4dm_papers:
            print("No ML4DM papers to post")
            sys.exit(0)

        # Create repository link to the output file
        repo_link = "https://github.com/${{ github.repository }}/blob/${{ github.ref_name }}/output/hepex.md"

        # Initialize notifier and post
        notifier = MattermostNotifier(webhook_url)
        success = notifier.post_ml4dm_findings(ml4dm_papers, repo_link)

        if success:
            print(f"Successfully posted {len(ml4dm_papers)} ML4DM papers to Mattermost")
        else:
            print("Failed to post to Mattermost")
            sys.exit(1)
        EOF

    - name: Publish to WordPress (iDMEU)
      env:
        WP_SITE_URL: ${{ secrets.WP_SITE_URL }}
        WP_USERNAME: ${{ secrets.WP_USERNAME }}
        WP_APP_PASSWORD: ${{ secrets.WP_APP_PASSWORD }}
      run: |
        # Check if WordPress credentials are available
        if [ -z "$WP_SITE_URL" ] || [ -z "$WP_USERNAME" ] || [ -z "$WP_APP_PASSWORD" ]; then
          echo "⚠️ WordPress credentials not configured - skipping WordPress publishing"
          echo "To enable WordPress publishing, add WP_SITE_URL, WP_USERNAME, and WP_APP_PASSWORD secrets"
          exit 0
        fi
        
        # Count papers for description
        PAPER_COUNT=$(tail -n +3 output/hepex.md | grep -c "^[^|]*|" || echo "0")
        export PAPER_COUNT
        
        uv run python << 'EOF'
        import os
        import sys
        from ciphr.src.wordpress_publisher import publish_research_insights

        # Get WordPress credentials from environment
        site_url = os.getenv('WP_SITE_URL', 'https://www.idmeu.org')
        username = os.getenv('WP_USERNAME')
        app_password = os.getenv('WP_APP_PASSWORD')
        paper_count = int(os.getenv('PAPER_COUNT', '0'))

        if not username or not app_password:
            print("ERROR: WordPress credentials not found in environment")
            sys.exit(1)

        # Publish to WordPress
        success = publish_research_insights(
            markdown_file_path='output/hepex.md',
            site_url=site_url,
            username=username,
            app_password=app_password,
            paper_count=paper_count
        )

        if success:
            print(f"✅ Successfully published research insights to {site_url}")
            print("📝 The iDMEU website has been updated with the latest research table")
        else:
            print("❌ Failed to publish to WordPress")
            print("The workflow will continue, but manual WordPress update may be needed")
            # Don't fail the workflow for WordPress issues
        EOF

    - name: Upload final insights
      uses: actions/upload-artifact@v4
      with:
        name: hep-ex-insights
        path: output/hepex*.md

    - name: Commit and push changes (if any)
      run: |
        git config user.name github-actions
        git config user.email github-actions@github.com
        # Only commit the final markdown file, not the intermediate JSON/txt files
        git add --force output/hepex*.md
        git commit -m "Update research insights table (hybrid approach)" || echo "No changes to commit"
        git push origin HEAD
