name: CIPhR Daily Research Ingestion (Hybrid)

# NOTE: This workflow requires the following secrets to be set:
# - PROD_BOT_KEY: SSH key for pushing to repository
# - GEMINI_API_KEY: Google Gemini API key for LLM analysis
# - MM_WEBHOOK_URL: Mattermost incoming webhook URL for ML4DM notifications (optional)
# 
# WordPress Integration (optional - workflow skips gracefully if not configured):
# - WP_USERNAME: WordPress username for iDMEU integration (required for WordPress)
# - WP_APP_PASSWORD: WordPress application password for iDMEU integration (required for WordPress) 
# - WP_SITE_URL: WordPress site URL (optional, defaults to https://www.idmeu.org)

on:
  schedule:
    # Runs every day at 7:00 AM UK time (7:00 UTC in winter, 8:00 AM in summer)
    - cron: '0 7 * * *'
  workflow_dispatch: # Allows manual triggering

jobs:
  collect_data:
    runs-on: ubuntu-latest
    outputs:
      papers-found: ${{ steps.data-collection.outputs.papers-found }}

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        ssh-key: ${{secrets.PROD_BOT_KEY}}

    - name: Install uv
      uses: astral-sh/setup-uv@v3
      with:
        version: "latest"
        
    - name: Set up Python
      run: uv python install 3.11
        
    - name: Install dependencies
      run: |
        uv sync --refresh
        
    - name: Run Data Collection
      id: data-collection
      run: |
        cd ${{ github.workspace }}
        uv run ciphr --mode collect --max_results 10 --output_filename hepex.md --tags hep-ex --verbose
        
        # Set output based on whether papers_data.json was created and has content
        if [ -f "output/papers_data.json" ]; then
          PAPER_COUNT=$(python3 -c "import json; data=json.load(open('output/papers_data.json')); print(len(data))" 2>/dev/null || echo "0")
          echo "papers-found=$PAPER_COUNT" >> $GITHUB_OUTPUT
          echo "Found $PAPER_COUNT papers for analysis"
        else
          echo "papers-found=0" >> $GITHUB_OUTPUT
          echo "No papers found"
        fi

    - name: Upload collected data
      if: steps.data-collection.outputs.papers-found > 0
      uses: actions/upload-artifact@v4
      with:
        name: collected-data
        path: |
          output/papers_data.json
          output/analysis_prompts.json

  analyze_papers:
    needs: collect_data
    runs-on: ubuntu-latest
    if: needs.collect_data.outputs.papers-found > 0

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        ssh-key: ${{secrets.PROD_BOT_KEY}}

    - name: Download collected data
      uses: actions/download-artifact@v4
      with:
        name: collected-data
        path: output/

    - name: Install uv
      uses: astral-sh/setup-uv@v3
      with:
        version: "latest"
        
    - name: Set up Python
      run: uv python install 3.11
        
    - name: Install dependencies
      run: |
        uv sync --refresh

    - name: Install uv
      uses: astral-sh/setup-uv@v3
      with:
        version: "latest"
        
    - name: Set up Python
      run: uv python install 3.11
        
    - name: Install dependencies
      run: |
        uv sync --refresh

    - name: Prepare papers for individual analysis
      run: |
        # Process each paper individually (using original proven approach)
        uv run python << 'EOF'
        import json
        import os

        # Load papers data
        with open('output/papers_data.json', 'r') as f:
            papers = json.load(f)

        print(f"Processing {len(papers)} papers individually...")

        # Create individual paper files for analysis
        for i, paper in enumerate(papers):
            print(f"Preparing paper {i+1}/{len(papers)}: {paper['title'][:50]}...")
            
            # Create individual paper file for this analysis
            individual_paper = {
                "title": paper["title"],
                "abstract": paper.get("abstract", ""),
                "combined_content": paper.get("combined_content", "")[:2000],  # Limit content size
                "arxiv_url": paper["arxiv_url"]
            }
            
            with open(f'paper_{i+1}.json', 'w') as f:
                json.dump(individual_paper, f, indent=2)
            
            print(f"Created paper_{i+1}.json for individual analysis")

        print("All papers prepared for individual analysis")
        EOF

    # Dynamic analysis steps - will scale based on number of papers found
    - name: Analyze Paper 1
      id: analyze-1
      if: needs.collect_data.outputs.papers-found >= 1
      uses: 'google-github-actions/run-gemini-cli@v0'
      with:
        gemini_api_key: '${{ secrets.GEMINI_API_KEY }}'
        prompt: |
          Analyze this physics research paper and answer the following questions. Respond with a JSON object with the exact question text as keys.
          
          Questions:
          1. "What is the main physics phenomenon studied by this paper"
          2. "Is this work related to dark matter searches? If yes, how?"
          3. "Does this paper present experimental results? If yes, what is the name of the experimental apparatus?"
          4. "Does this paper use ML techniques for dark matter searches? And if yes, list the main ML techniques used in this paper"
          
          Response format:
          {
            "What is the main physics phenomenon studied by this paper": "Your answer here",
            "Is this work related to dark matter searches? If yes, how?": "Your answer here",
            "Does this paper present experimental results? If yes, what is the name of the experimental apparatus?": "Your answer here",
            "Does this paper use ML techniques for dark matter searches? And if yes, list the main ML techniques used in this paper": "Your answer here"
          }

          Paper to analyze:
          $(cat paper_1.json)

    - name: Analyze Paper 2
      id: analyze-2
      if: needs.collect_data.outputs.papers-found >= 2
      uses: 'google-github-actions/run-gemini-cli@v0'
      with:
        gemini_api_key: '${{ secrets.GEMINI_API_KEY }}'
        prompt: |
          Analyze this physics research paper and answer the following questions. Respond with a JSON object with the exact question text as keys.
          
          Questions:
          1. "What is the main physics phenomenon studied by this paper"
          2. "Is this work related to dark matter searches? If yes, how?"
          3. "Does this paper present experimental results? If yes, what is the name of the experimental apparatus?"
          4. "Does this paper use ML techniques for dark matter searches? And if yes, list the main ML techniques used in this paper"
          
          Response format:
          {
            "What is the main physics phenomenon studied by this paper": "Your answer here",
            "Is this work related to dark matter searches? If yes, how?": "Your answer here",
            "Does this paper present experimental results? If yes, what is the name of the experimental apparatus?": "Your answer here",
            "Does this paper use ML techniques for dark matter searches? And if yes, list the main ML techniques used in this paper": "Your answer here"
          }

          Paper to analyze:
          $(cat paper_2.json)

    - name: Analyze Paper 3
      id: analyze-3
      if: needs.collect_data.outputs.papers-found >= 3
      uses: 'google-github-actions/run-gemini-cli@v0'
      with:
        gemini_api_key: '${{ secrets.GEMINI_API_KEY }}'
        prompt: |
          Analyze this physics research paper and answer the following questions. Respond with a JSON object with the exact question text as keys.
          
          Questions:
          1. "What is the main physics phenomenon studied by this paper"
          2. "Is this work related to dark matter searches? If yes, how?"
          3. "Does this paper present experimental results? If yes, what is the name of the experimental apparatus?"
          4. "Does this paper use ML techniques for dark matter searches? And if yes, list the main ML techniques used in this paper"
          
          Response format:
          {
            "What is the main physics phenomenon studied by this paper": "Your answer here",
            "Is this work related to dark matter searches? If yes, how?": "Your answer here",
            "Does this paper present experimental results? If yes, what is the name of the experimental apparatus?": "Your answer here",
            "Does this paper use ML techniques for dark matter searches? And if yes, list the main ML techniques used in this paper": "Your answer here"
          }

          Paper to analyze:
          $(cat paper_3.json)

    - name: Analyze Paper 4
      id: analyze-4
      if: needs.collect_data.outputs.papers-found >= 4
      uses: 'google-github-actions/run-gemini-cli@v0'
      with:
        gemini_api_key: '${{ secrets.GEMINI_API_KEY }}'
        prompt: |
          Analyze this physics research paper and answer the following questions. Respond with a JSON object with the exact question text as keys.
          
          Questions:
          1. "What is the main physics phenomenon studied by this paper"
          2. "Is this work related to dark matter searches? If yes, how?"
          3. "Does this paper present experimental results? If yes, what is the name of the experimental apparatus?"
          4. "Does this paper use ML techniques for dark matter searches? And if yes, list the main ML techniques used in this paper"
          
          Response format:
          {
            "What is the main physics phenomenon studied by this paper": "Your answer here",
            "Is this work related to dark matter searches? If yes, how?": "Your answer here",
            "Does this paper present experimental results? If yes, what is the name of the experimental apparatus?": "Your answer here",
            "Does this paper use ML techniques for dark matter searches? And if yes, list the main ML techniques used in this paper": "Your answer here"
          }

          Paper to analyze:
          $(cat paper_4.json)

    - name: Analyze Paper 5
      id: analyze-5
      if: needs.collect_data.outputs.papers-found >= 5
      uses: 'google-github-actions/run-gemini-cli@v0'
      with:
        gemini_api_key: '${{ secrets.GEMINI_API_KEY }}'
        prompt: |
          Analyze this physics research paper and answer the following questions. Respond with a JSON object with the exact question text as keys.
          
          Questions:
          1. "What is the main physics phenomenon studied by this paper"
          2. "Is this work related to dark matter searches? If yes, how?"
          3. "Does this paper present experimental results? If yes, what is the name of the experimental apparatus?"
          4. "Does this paper use ML techniques for dark matter searches? And if yes, list the main ML techniques used in this paper"
          
          Response format:
          {
            "What is the main physics phenomenon studied by this paper": "Your answer here",
            "Is this work related to dark matter searches? If yes, how?": "Your answer here",
            "Does this paper present experimental results? If yes, what is the name of the experimental apparatus?": "Your answer here",
            "Does this paper use ML techniques for dark matter searches? And if yes, list the main ML techniques used in this paper": "Your answer here"
          }

          Paper to analyze:
          $(cat paper_5.json)

    - name: Combine individual results
      run: |
        # Combine all individual analysis results (updated for up to 10 papers)
        uv run python << 'EOF'
        import json
        import os

        # Load papers count
        with open('output/papers_data.json', 'r') as f:
            papers_count = len(json.load(f))

        print(f"Combining results for {papers_count} papers")

        # Collect individual results based on actual paper count (max 5)
        results = []
        step_outputs = [
            '''${{ steps.analyze-1.outputs.summary }}''',
            '''${{ steps.analyze-2.outputs.summary }}''',
            '''${{ steps.analyze-3.outputs.summary }}''',
            '''${{ steps.analyze-4.outputs.summary }}''',
            '''${{ steps.analyze-5.outputs.summary }}'''
        ]

        for i in range(papers_count):
            print(f"Processing result {i+1}")
            
            if i < len(step_outputs):
                result = step_outputs[i]
            else:
                result = '{"error": "No analysis performed - exceeded maximum papers"}'
            
            if not result or result.strip() == '':
                result = '{"error": "Empty LLM response"}'
            
            results.append(result)
            print(f"Added result {i+1}: {result[:100]}...")

        # Save combined results
        with open('output/llm_results.txt', 'w') as f:
            f.write("---PAPER---".join(results))

        print(f"Saved combined results for {len(results)} papers")
        
        # Debug output
        print("=== Combined Results Preview ===")
        with open('output/llm_results.txt', 'r') as f:
            content = f.read()
            print(content[:500] + "..." if len(content) > 500 else content)
        print("=== End Preview ===")
        EOF

    - name: Upload analysis results
      uses: actions/upload-artifact@v4
      with:
        name: llm-analysis-results
        path: output/llm_results.txt

    - name: Upload analysis results
      uses: actions/upload-artifact@v4
      with:
        name: analysis-results
        path: |
          output/papers_data.json
          output/analysis_prompts.json
          output/llm_results.txt

  process_results:
    needs: [collect_data, analyze_papers]
    runs-on: ubuntu-latest
    if: needs.collect_data.outputs.papers-found > 0

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        ssh-key: ${{secrets.PROD_BOT_KEY}}

    - name: Install uv
      uses: astral-sh/setup-uv@v3
      with:
        version: "latest"
        
    - name: Set up Python
      run: uv python install 3.11
        
    - name: Install dependencies
      run: |
        uv sync --refresh
        # Install pandoc for robust markdown to HTML conversion
        sudo apt-get update
        sudo apt-get install -y pandoc

    - name: Download analysis results
      uses: actions/download-artifact@v4
      with:
        name: analysis-results
        path: output/

    - name: Process results and generate markdown
      run: |
        # Run result processing phase
        uv run ciphr --mode process --output_dir output --output_filename hepex.md --verbose

    - name: Check for ML4DM papers and prepare Mattermost notification
      id: ml4dm-check
      run: |
        # Run ML4DM analysis
        uv run python << 'EOF'
        import json
        import os
        import sys
        from ciphr.src.mattermost_notifier import check_ml4dm_papers, MattermostNotifier

        # Load papers data
        with open('output/papers_data.json', 'r') as f:
            papers_data = json.load(f)

        # Load LLM results
        with open('output/llm_results.txt', 'r') as f:
            content = f.read().strip()

        # Parse LLM results (same logic as in ciphr_hybrid.py)
        PAPER_SEPARATOR = "---PAPER---"
        if PAPER_SEPARATOR in content:
            llm_results = content.split(PAPER_SEPARATOR)
            llm_results = [result.strip() for result in llm_results if result.strip()]
        else:
            llm_results = [content]

        # Ensure we have the right number of results
        if len(llm_results) < len(papers_data):
            while len(llm_results) < len(papers_data):
                llm_results.append('{"error": "No LLM result available"}')
        elif len(llm_results) > len(papers_data):
            llm_results = llm_results[:len(papers_data)]

        print(f"Analyzing {len(papers_data)} papers for ML4DM usage...")

        # Check for ML4DM papers
        ml4dm_papers = check_ml4dm_papers(papers_data, llm_results)

        # Save results
        with open('output/ml4dm_papers.json', 'w') as f:
            json.dump(ml4dm_papers, f, indent=2)

        # Set output for next step
        has_ml4dm = len(ml4dm_papers) > 0
        print(f"Found {len(ml4dm_papers)} ML4DM papers")
        
        # Write to GitHub Actions output
        with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
            f.write(f"has-ml4dm-papers={'true' if has_ml4dm else 'false'}\n")
            f.write(f"ml4dm-count={len(ml4dm_papers)}\n")

        if has_ml4dm:
            print("ML4DM papers found - will post to Mattermost")
        else:
            print("No ML4DM papers found - skipping Mattermost notification")
        EOF

    - name: Post to Mattermost if ML4DM papers found
      if: steps.ml4dm-check.outputs.has-ml4dm-papers == 'true'
      env:
        MM_WEBHOOK_URL: ${{ secrets.MM_WEBHOOK_URL }}
      run: |
        uv run python << 'EOF'
        import json
        import os
        import sys
        from ciphr.src.mattermost_notifier import MattermostNotifier

        # Get webhook URL from environment
        webhook_url = os.getenv('MM_WEBHOOK_URL')
        if not webhook_url:
            print("ERROR: MM_WEBHOOK_URL not found in environment")
            sys.exit(1)

        # Load ML4DM papers
        with open('output/ml4dm_papers.json', 'r') as f:
            ml4dm_papers = json.load(f)

        if not ml4dm_papers:
            print("No ML4DM papers to post")
            sys.exit(0)

        # Create repository link to the output file
        repo_link = "https://github.com/${{ github.repository }}/blob/${{ github.ref_name }}/output/hepex.md"

        # Initialize notifier and post
        notifier = MattermostNotifier(webhook_url)
        success = notifier.post_ml4dm_findings(ml4dm_papers, repo_link)

        if success:
            print(f"Successfully posted {len(ml4dm_papers)} ML4DM papers to Mattermost")
        else:
            print("Failed to post to Mattermost")
            sys.exit(1)
        EOF

    - name: Publish to WordPress (iDMEU)
      env:
        WP_SITE_URL: ${{ secrets.WP_SITE_URL }}
        WP_USERNAME: ${{ secrets.WP_USERNAME }}
        WP_APP_PASSWORD: ${{ secrets.WP_APP_PASSWORD }}
      run: |
        # Check if WordPress credentials are available
        if [ -z "$WP_SITE_URL" ] || [ -z "$WP_USERNAME" ] || [ -z "$WP_APP_PASSWORD" ]; then
          echo "⚠️ WordPress credentials not configured - skipping WordPress publishing"
          echo "To enable WordPress publishing, add WP_SITE_URL, WP_USERNAME, and WP_APP_PASSWORD secrets"
          exit 0
        fi
        
        # Count papers for description
        PAPER_COUNT=$(tail -n +3 output/hepex.md | grep -c "^[^|]*|" || echo "0")
        export PAPER_COUNT
        
        uv run python << 'EOF'
        import os
        import sys
        from ciphr.src.wordpress_publisher import publish_research_insights

        # Get WordPress credentials from environment
        site_url = os.getenv('WP_SITE_URL', 'https://www.idmeu.org')
        username = os.getenv('WP_USERNAME')
        app_password = os.getenv('WP_APP_PASSWORD')
        paper_count = int(os.getenv('PAPER_COUNT', '0'))

        if not username or not app_password:
            print("ERROR: WordPress credentials not found in environment")
            sys.exit(1)

        # Publish to WordPress
        success = publish_research_insights(
            markdown_file_path='output/hepex.md',
            site_url=site_url,
            username=username,
            app_password=app_password,
            paper_count=paper_count
        )

        if success:
            print(f"✅ Successfully published research insights to {site_url}")
            print("📝 The iDMEU website has been updated with the latest research table")
        else:
            print("❌ Failed to publish to WordPress")
            print("The workflow will continue, but manual WordPress update may be needed")
            # Don't fail the workflow for WordPress issues
        EOF

    - name: Upload final insights
      uses: actions/upload-artifact@v4
      with:
        name: hep-ex-insights
        path: output/hepex*.md

    - name: Commit and push changes (if any)
      run: |
        git config user.name github-actions
        git config user.email github-actions@github.com
        # Only commit the final markdown file, not the intermediate JSON/txt files
        git add --force output/hepex*.md
        git commit -m "Update research insights table (hybrid approach)" || echo "No changes to commit"
        git push origin HEAD
